# Beyond-Demonstration
 CSE-598 Perception in Robotics Project ASU

## Introduction
This project aims to address the challenge of enhancing Human-Robot Collaboration by enabling robots to learn new tasks more efficiently through Learning from Demonstration (LfD). Specifically, we focus on improving the D-REX algorithm, which utilizes ranked demonstrations for Inverse Reinforcement Learning (IRL), by proposing three significant improvements to reduce performance variability and increase the average rewards.

### Team Members
- Prabin Kumar Rath - *prath4@asu.edu*
- Sok Yan Poon - *spoon3@asu.edu*
- Pranit Sehgal - *pvsehgal@asu.edu*
- Nilay Yilmaz - *nyilmaz3@asu.edu*

## Background
Imitation Learning and Inverse Reinforcement Learning (IRL) are pivotal in teaching robots to mimic expert behavior. Our work builds upon the D-REX algorithm, which generates automatically ranked demonstrations by injecting different levels of noise into expert trajectories. Our objective is to enhance this algorithm to achieve better-than-demonstrator performance.

## Our Approach
We introduce three main enhancements to the D-REX algorithm:
1. **Improved Preference Model:** Modification of the preference model to include a discount factor and a preference noise term, improving stability in the reward learning process.
2. **Fixed Horizon Rollouts:** Implementation of fixed horizon rollouts to eliminate inductive bias and encourage the reward function to encode only environment-derived information.
3. **Single Reward Network:** Adoption of a single reward network with input normalization and output scaling, reducing the parameter count and enhancing regularization.


 Implementation of [T-REX](http://proceedings.mlr.press/v97/brown19a/brown19a.pdf) and [D-REX](https://arxiv.org/pdf/1907.03976.pdf) IRL algorithm that learns a reward capturing the expert's intention from the demonstrations. The learnt reward can be used to train a policy that performs better than the demonstrations. Based on [stable_baselines3](https://stable-baselines3.readthedocs.io/en/master/#) and [imitation](https://imitation.readthedocs.io/en/latest/index.html) packages.

 Given a dataset of ranked sub-optimal demonstrations, a state-dependent reward function can be recovered by training a neural network optimized on the preference of better demonstrations over the worst. The Bradley-Terry and Luce-Shephard [model](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model) of choice is used to train such reward models from preferences. Ranked trajectories can be generated by injecting different levels of noise into a [BC](https://www.ri.cmu.edu/pub_files/pub3/pomerleau_dean_1991_1/pomerleau_dean_1991_1.pdf) policy trained on the demonstrations.
 
 Example of 3 ranked trajectories generated by noise injection.

| High Noise | Mid Noise | No Noise |
| ------- | ------- | ------- |
| ![alt text 1](media/noise_high.gif "High Noise") | ![alt text 2](media/noise_mid.gif "Mid Noise") | ![alt text 3](media/noise_low.gif "No Noise") |
| Least Preferred | More Preferred | Most Preferred |
 

 # Changes to Baseline
We implement the IRL algorithm using the tools available in the `imitation` library. Notable changes from the paper's implementation are
- Luce preference with discount_factor, noise_prob, clipped reward differences (ideas from [DRLHP](https://arxiv.org/pdf/1706.03741)): Comes with imitation library
- Mixed sampling: New preference dataset generated every epoch
- Fixed horizon rollouts for ranked_trajectories: Horizon length is 1000 steps
- Input normalization in reward network (similar to batch normalization): Comes with imitation library
- Single reward function No ensemble
- Reward scaling with tanh, optimized with AdamW: Scaled reward improves stability
- Entropy regularized actor-critic policy for BC: Comes with imitation library

Other possible improvements
- Use custom rewards (rnn, attention)
- A better preference loss ([aLRP](https://arxiv.org/pdf/2009.13592))

# Results
Better than demonstrator performance was observed for HalfCheetah-v3 environment. For Hopper-v3, we achieved equal to demonstration performance.
| Demonstration | Learned Policy |
| ------- | ------- |
| <img src="media/hc_demo.gif" width="300" height="180"> | <img src="media/hc_better.gif" width="300" height="180">  |
| <img src="media/hp_demo.gif" width="300" height="180"> | <img src="media/hp_better.gif" width="300" height="180">  |

Ground Truth reward to Predicted reward correlation (unscaled)
| Hopper | HalfCheetah |
| ------- | ------- |
| <img src="media/hopper_rw_exp.png" width="300" height="300"> | <img src="media/halfch_rw_exp.png" width="300" height="300">  |

## Technologies Used
- **Programming Language:** Python
- **Libraries:** TensorFlow, OpenAI Gym, stable_baselines3, imitation
- **Algorithms:** T-REX, D-REX, Proximal Policy Optimization (PPO)

## How to Run
To reproduce our results or apply our approach to your datasets:
1. Clone the repository.
2. Install the required libraries: `pip install -r requirements.txt`.
3. Run the provided Jupyter notebooks or Python scripts for behavior cloning, noise injection, reward extrapolation, and reinforcement learning in your chosen environments.

## Conclusion and Future Work
Our enhancements to the D-REX algorithm represent a step forward in Imitation Learning and IRL by not only reducing the variability in learned policy performance but also by outperforming the demonstrator in complex tasks. Future work includes extending our approach to more complex environments and integrating the SSRR algorithm for further improvements.

## Acknowledgments
Our thanks to the authors of the D-REX and T-REX algorithms for their foundational work, which inspired our research.

## Contact
For further inquiries or collaboration, feel free to contact any of the team members via email.

